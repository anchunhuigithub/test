                                                       hive study file
1,version 1
    常用命令: 
        set hive.cli.print.current.db=true  设置出 cli的db名称
        set hive.cli.print.header=true   设置 cli的  查询结果显示出列名称
        create database databasename;      创建数据库
        create table tablename (filename type,filename1 type)j;      创建表
    显示分区: 
        show partitions tablename;
    添加分区: 
        alter table tablename add [if not exists] partition partition_1 [location 'location'] partition partition_2 ['location'] ...;
        example:
        ALTER TABLE iteblog ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'
        PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';
    删除分区: 
        alter table table_name drop [if exists] partition partition_spec[,paritiont]
        example:
        ALTER TABLE iteblog DROP PARTITION (dt='2008-08-08', country='us');
    修改分区: 
        ALTER TABLE iteblog PARTITION (dt='2008-08-08') SET LOCATION "new location"; 更改数据路径, 原始数据并没有删除. 
        ALTER TABLE iteblog PARTITION (dt='2008-08-08') RENAME TO PARTITION (dt='20080808');
    添加列: 
        ALTER TABLE iteblog ADD COLUMNS (col_name STRING);
    修改列: 
            CREATE TABLE iteblog (a int, b int, c int);
             
            // will change column a's name to a1
            ALTER TABLE iteblog CHANGE a a1 INT; 
             
            // will change column a's name to a1, a's data type to string, and put it after column b. The new table's structure is: b int, a1 string, c int
            ALTER TABLE iteblog CHANGE a a1 STRING AFTER b; 
             
            // will change column b's name to b1, and put it as the first column. The new table's structure is: b1 int, a string, c int
            ALTER TABLE iteblog CHANGE b b1 INT FIRST; 


            命令1:
            # 把一个查询结果作为插入的内容插入到一张表里去 over 表示覆盖(先清空表，再插入)，into 表示直接插入。
                 insert  overwrite/into table_target select * from table_src;
            # 创建表的同时直接插入查询结果, 应用场景, 查询结果过多, 不进行控制台打印直接插入到表里去
                 create table table_1 as select * from table_2 where table_2.age>20

            # 使用group by 进行嵌套查询 需要在后面添加as tablealis 不然会报错 这是在mysql的语法 
                 select count(1) from (select _from from table group by _from) as t;
                 select count(distinct(1)) from table;

            # 在hive中的使用 的性能比较
    修改表属性: 
        alter table iteblog set TBLPROPERTIES ('EXTERNAL'='TRUE');
        alter table iteblog set TBLPROPERTIES ('EXTERNAL'='FALSE');
  
    表的重新命名: 
       ALTER TABLE iteblog RENAME TO new_table_name

命令2:
# 动态分区和静态分区
   静态分区:  一次导入到一个分区, 如果对应文件夹没有创建, 自动创建. 
   insert into table table_1 partition (year=2018,month=08,day=08) select * from table2 where year=2018 and month =08 and day =08
    静态分区一次导入到多个分区
    from table1 
    insert into table table2 partition (year=2018,month=08,day=08)
    select * where year=2018 and month=08 and day =08
    insert into table table2 partition (year=2018,month=08,day=09)
    select * where year=2018 and month=08 and day=09
   
   动态分区:
   动态分区默认是关闭的, 需要手动打开 容许动态分区, 容许所有列作为动态分区字段. hive.exec.dynamic.partition.mode 设置为nostrict
   根据字段自动创建分区,根据查询结果的后三位进行动态分区. 
   insert into table table2 partition(year,month,day) select * from t3.name,t3.age,.....,t3.year,t3.mohth,t3.day from table3 t3
   动态分区一次插入多张表中 from 字段放到前面可以插入到多张表中:  动态分区和静态分区结合使用 
   from table1
   insert into table table2 partition (year,month,day)
   select name,age,year,month,day
   insert into table table3 partition (year=2018,month=08,day)
   select name,age,year,month,day where table1.year=2018,month=08

命令3:
   join 操作
   inner join 内连接, 只用在两张表都符合where条件才会作为结果显示 ,join 只支持等值连接 on后面的条件使用'=’
   select a.f1,a.f2,b.f2 from table1 a join table2 b on a.f1=b.f1
   
   *** 待定  todo
   left outer join  左外连接  左边的表的记录符合where条件的都显示, 对应的右边的表

   union all
   把表合并到一起操作



#   hive 中array，map，struct字段类型的使用场景
	1，array
	        以电影数据为列
		数据维度
		导演，编剧，类型，主演，year，month，市场，score
		其中导演，编剧，类型和主演为array类型的
		创建表
		create table movie_message(
		    id int,
		    title string,
		    daoyan array<string>,
		    bianju array<string>,
		    leixing array<string>,
		    zhuyan array<string>,
		    year int,
	  	    month int,
            shichang int,
	        disnum int,
	        score float
		)
		comment "this table about movie's message"
		row format delimited fields terminated by ","
		collection items terminated by '/';
          加载本地数据：
	  load data local inpath "/home/master/mycode/new_movies_load.csv" into table movie_message;
	  内嵌查询及统计：
          select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;
		结果：
		        传记    194
		        儿童    18
			冒险    242
			剧情    1490
			动作    252
			动画    106
			历史    208
			古装    9
			同性    84
			喜剧    618
			奇幻    178
			家庭    130
			恐怖    152
			悬念    2
			悬疑    386

	保存结果到本地文件:
	insert overwrite local directory "/home/master/mycode/movie_leixing"
	row format delimited fields terminated by "\t"
	select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;

	2,map
	场景：
	create table people_movie(
		 name string,
		 movie map<string,string> )
		 row format delimited fields terminated by "\t"
		 collection items terminated by ","
		 map keys terminated by ":";
	加载数据：
		load data local inpath "/home/master/map" into table people_movie;

	hive> select * from people_movie;
	OK
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	Time taken: 0.148 seconds, Fetched: 4 row(s)
	hive> select movie['ABC'] from people_movie;
	OK
	2016-05
	NULL
	2016-05
	NULL
	使用explode关键字进行查询
	hive> select explode(movie) as (m_name,m_time) from people_movie;
	OK
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	使用explode和lateral view 结合查询
	hive> select name,mo,time from people_movie lateral view explode(movie) movie as mo,time;
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	
	3，Structs
	类似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分
	ABC     1254:7.4
	DEF     256:4.9
	XYZ     456:5.4
	
	创建表：
	create table movie_score(
    	> name string,
    	> info struct<number:int,score:float>)row format delimited fields terminated by "\t"
   	> collection items terminated by ":";


	hive> select * from movie_score;
	ABC     {"number":1254,"score":7.4}
	DEF     {"number":256,"score":4.9}
	XYZ     {"number":456,"score":5.4}
	hive> select info.number,info.score from movie_score;
	1254    7.4
	256     4.9
	456     5.4

	4，collect_set函数
        该函数的作用是将某字段的值进行去重汇总，产生Array类型字段
		
		select * from test;

		1       A
		1       C
		1       B
		2       B
		2       C
		2       D
		3       B
		3       C
		3	d
	
	 select id,collect_set(name) from test group by id;
	  
	  1       ["A","C","B"]
	  2       ["B","C","D"]
	  3       ["B","C","D"]
********************************************************************************优化************************************************
 
 设置hive本地模式运行
    set hive.exec.mode.local.auto=true
    有些语句就不会促发mapreduce执行, 比如selct * from table1;  select * from table where day=10 (分区表)
 
 尽量使用group by 进行分组操作, 效率高
 
 jvm重用

 http://lxw1234.com/archives/2016/04/632.htm
 orc 索引row group index vs bloom filter index
 hive 的orc文件格式有着很高的压缩比, 还通过一个内置的轻量级索引, 提升查询性能, row group index 




********************************************************************************易混知识点****************************************
    hive 中order by, sort by, distribute by 和cluster by的介绍
    https://www.iteblog.com/archives/1534.html 有详细的介绍
    
    hive中的order by和mysql中的order by是一致的, 按照某一列, 或多列进行排序, 可以指定是正序或者是倒序, 它保证全局有序, 进行order by的时候会把所有数据发送到一个reducer中
    , 在大数据量的情况下可能不能接受, 这个操作最后会参数一个文件. 
    
    sort by 
    只能保证在同一个reduce中的数据按照指定的字段进行排序, 使用sort by 可以通过指定reduce的个数输出更多的数据, 对输出的数据再进行归并排序, 即可得到全部的结果, 需要注意的是n个reduce
    处理的数据范围是可以重叠的, 最后排序完的n个文件之间的数据范围是重叠的. 
    
    distribute by
    按照指定的字段将数据划分到不同的reduce中, 可以保证每个reduce中的数据范围不重叠, 每个分区内的数据是没有排序的.
    distribute by 一般和sort by一起使用: 进行reduce内部排序. 

    
    cluster by 具有distribute by 的功能, 并且可以在reduce中进行排序, 每个reduce内的数据是有序的, 可以达到全局有序的结果
********************************************************************************hive 中reduces个数是如何计算的************************
    参考文献: https://www.iteblog.com/archives/1697.html
 
 hive.exec.reducers.bytes.per.reducer
 此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是1G(1,000,000,000)；而从Hive 0.14.0开始，默认值变成了256M(256,000,000)，
 可以参见HIVE-7158和HIVE-7917。这个参数的含义是每个Reduce处理的字节数。比如输入文件的大小是1GB，那么会启动4个Reduce来处理数据 
 
 hive.exec.reducers.max 指定最多启动的reducer的个数
    此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是999；而从Hive 0.14.0开始，默认值变成了1009；可以参见HIVE-7158和HIVE-7917。
    这个参数的含义是最多启动的Reduce个数。比如input size/hive.exec.reducers.bytes.per.reducer>hive.exec.reducers.max，
    那么Hive启动的Reduce个数为hive.exec.reducers.max；反之为input size/hive.exec.reducers.bytes.per.reducer。
    这个参数只有在mapred.reduce.tasks/mapreduce.job.reduces设置为负数的时候才有效
 
 mapred.reduce.tasks/mapreduce.job.reduces
    此参数从Hive 0.1.0开始引入。默认值是-1。此参数的含义是Reduce的个数，典型的情况是设置成接近可用节点的质数。
    如果mapred.job.tracker的值是local此参数将会被忽略。在Hadoop中此参数的默认值是1；而在Hive中默认值是-1。
    通过将此参数设置为-1，Hive将自动计算出应该启动多少个Reduce。

 计算reduce个数的代码片段: 
    double bytes = Math.max(totalInputFileSize, bytesPerReducer); 
    int reducers = (int) Math.ceil(bytes / bytesPerReducer);
    reducers = Math.max(1, reducers);
    reducers = Math.min(maxReducers, reducers);   
 reduce 最小为1, 最大为maxReduces 就是hive.exec.reducers.max的值
    上面的代码仅仅是根据输入计算出reduce的个数, 最终的设置是通过设置setNumberOfReducers函数决定的
    如果job.getNumReduceTasks()>0，也就是mapred.reduce.tasks/mapreduce.job.reduces参数的值大于0，
    此时直接取mapred.reduce.tasks/mapreduce.job.reduces的值作为Reduce的个数；
    否则将会使用上面estimateNumberOfReducers函数估算Reduce个数，
    最后都是通过rWork.setNumReduceTasks(reducers)设置Reduce的个数。
总结: 
    1、Reduce的个数对整个作业的运行性能有很大影响。如果Reduce设置的过大，那么将会产生很多小文件，对NameNode会产生一定的影响，
       而且整个作业的运行时间未必会减少；如果Reduce设置的过小，那么单个Reduce处理的数据将会加大，很可能会引起OOM异常。
　　2、如果设置了mapred.reduce.tasks/mapreduce.job.reduces参数，那么Hive会直接使用它的值作为Reduce的个数；
　　3、如果mapred.reduce.tasks/mapreduce.job.reduces的值没有设置（也就是-1），那么Hive会根据输入文件的大小估算出Reduce的个数。
       根据输入文件估算Reduce的个数可能未必很准确，因为Reduce的输入是Map的输出，而Map的输出可能会比输入要小，所以最准确的数根据Map的输出估算Reduce的个数。

**********************************************************************************hive 分析窗口函数***************************************
   引用文献: http://lxw1234.com/archives/2015/04/181.htm  
             http://lxw1234.com/archives/2015/04/176.htm window 子句: 
    
   数据的准备和下面的一样: 
    SELECT cookieid,
    createtime,
    pv,
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1 
    SUM(pv) OVER(PARTITION BY cookieid) AS pv3,								--分组内所有行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv4,   --当前行+往前3行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv5,    --当前行+往前3行+往后1行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv6   ---当前行+往后所有行  
    FROM lxw1234;
     
    cookieid createtime     pv      pv1     pv2     pv3     pv4     pv5      pv6 
    -----------------------------------------------------------------------------
    cookie1  2015-04-10      1       1       1       26      1       6       26
    cookie1  2015-04-11      5       6       6       26      6       13      25
    cookie1  2015-04-12      7       13      13      26      13      16      20
    cookie1  2015-04-13      3       16      16      26      16      18      13
    cookie1  2015-04-14      2       18      18      26      17      21      10
    cookie1  2015-04-15      4       22      22      26      16      20      8
    cookie1  2015-04-16      4       26      26      26      13      13      4
    
    如果不置顶rows between 那么默认从起点到当前行, 若果不指定order by 将分组内所有数据累加
    理解rows between的含义为: 
    preceding : 往前
    following : 往后
    current_row : 当前行
    unbounded : 起点 unbounded preceding:  表示从前面的起点, unbounded following : 表示到后面的终点 
    其它的AVG, min, max, 和sum的用法一样. 



    ntile,row_number,rank,dense_rank
    注意序列函数不支持window子句
            cookie1,2015-04-10,1
            cookie1,2015-04-11,5
            cookie1,2015-04-12,7
            cookie1,2015-04-13,3
            cookie1,2015-04-14,2
            cookie1,2015-04-15,4
            cookie1,2015-04-16,4
            cookie2,2015-04-10,2
            cookie2,2015-04-11,3
            cookie2,2015-04-12,5
            cookie2,2015-04-13,6
            cookie2,2015-04-14,3
            cookie2,2015-04-15,9
            cookie2,2015-04-16,7
             
            CREATE EXTERNAL TABLE lxw1234 (
            cookieid string,
            createtime string,   --day 
            pv INT
            ) ROW FORMAT DELIMITED 
            FIELDS TERMINATED BY ',' 
            stored as textfile location '/tmp/lxw11/';
   ntile(n) 用于将分组数据按照顺序切分为n片, 返回当前切片值, ntile不支持rows between  
   NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
   统计一个cookie pv最多的前三分之一的天, 
           select cookieid,createtime,pv,ntile(3) over(partition by cookieid order by pv desc) as rn from table;
           嵌套一下: 只要每一个cookieid的前三天的数据
        select * from (
             select cookieid,createtimr,pv,ntile(3) over(partition by cookieid order by pv desc) as rn from table
        )as tamp where tamp.rn=1;
        结果为:
        cookie1	2015-04-12	7	1
        cookie1	2015-04-11	5	1
        cookie1	2015-04-16	4	1
        cookie2	2015-04-15	9	1
        cookie2	2015-04-16	7	1
        cookie2	2015-04-13	6	1
   row_number() 从1开始, 按照顺序, 生成分组内记录的序列
   比如按照pv进行倒叙排序, 生成每天的pv名次 即使排名相等也会有顺序 1, 2, 3, 4, 5
        select cookieid,createtime,pv,row_number() over(partition by createtime order by pv desc) as rn from test1;
        按照时间找到每天的pv最大的cookieid 的相关记录
        select * from (select cookieid,createtime,pv,row_number() over(partition by createtime order by pv desc) as rn from test1) as tamp where tamp.rn=1;
   rank() 生成数据项在分组中的排名, 排名相等会在名次中留下空位 1, 2, 3, 3, 5
        select cookieid,createtime,pv,rank() over (partition by cookieid order by pv desc) as rn from test1;
   dense_rank() 和rank的区别, 排名相等的不会在次序的字段中留下空位, 1, 2, 3, 3, 4
        select cookieid,createtime,pv,dense_rank() over(partition by cookieid order by pv desc) as rn from test1; 
   








