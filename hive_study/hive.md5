                                                       hive study file

0,hive 安装 
    基本步骤很简单, 配置元数据存放路径为mysql

    ref: https://www.cnblogs.com/kinginme/p/7233315.html
        http://www.cnblogs.com/kinginme/p/7249533.html
    1, mysql 设置访问权限的bind 路径 /etc/mysql/mysql.conf.d (ubuntu) 
    2, 添加新的用户 hadoop 
        CREATE USER  'hadoop'@'%'  IDENTIFIED BY 'mysql';
        GRANT ALL PRIVILEGES ON  *.* TO 'hadoop'@'%' WITH GRANT OPTION;
        flush privileges;
    3,原来Hive2需要hive元数据库初始化
    schematool -dbType mysql -initSchema

1,version 1
    常用命令: 
        set hive.cli.print.current.db=true  设置出 cli的db名称
        set hive.cli.print.header=true   设置 cli的  查询结果显示出列名称
        create database databasename;      创建数据库
        create table tablename (filename type,filename1 type);      创建表
        创建内部表
        create table tablename(id int comment '',name string comment '')comment '' row format delimited fields terminated by "|" stored as TEXTFILE
        创建外部分区表
        create external table tablename(id int comment '',name string comment '')partitioned by (yyyymmdd string) row format delimited fields terminated by "|" stored as TEXTFILE location '/user/hive/warehouse/dataname/20181220'
        创建外部表 指定分区字段, 字段分割符号, 存储格式, 存放路径
    显示分区: 
        show partitions tablename;
    添加分区: 
        alter table tablename add [if not exists] partition partition_1 [location 'location'] partition partition_2 ['location'] ...;
        example:
        ALTER TABLE iteblog ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808'
        PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';
    删除分区: 
        alter table table_name drop [if exists] partition partition_spec[,paritiont]
        example:
        ALTER TABLE iteblog DROP PARTITION (dt='2008-08-08', country='us');
    修改分区: 
        ALTER TABLE iteblog PARTITION (dt='2008-08-08') SET LOCATION "new location"; 更改数据路径, 原始数据并没有删除. 
        ALTER TABLE iteblog PARTITION (dt='2008-08-08') RENAME TO PARTITION (dt='20080808');
    添加列: 
        ALTER TABLE iteblog ADD COLUMNS (col_name STRING);
    修改列: 
            CREATE TABLE iteblog (a int, b int, c int);
             
            // will change column a's name to a1
            ALTER TABLE iteblog CHANGE a a1 INT; 
             
            // will change column a's name to a1, a's data type to string, and put it after column b. The new table's structure is: b int, a1 string, c int
            ALTER TABLE iteblog CHANGE a a1 STRING AFTER b; 
             
            // will change column b's name to b1, and put it as the first column. The new table's structure is: b1 int, a string, c int
            ALTER TABLE iteblog CHANGE b b1 INT FIRST; 


            命令1:
            # 把一个查询结果作为插入的内容插入到一张表里去 over 表示覆盖(先清空表，再插入)，into 表示直接插入。
                 insert  overwrite/into table_target select * from table_src;
            # 创建表的同时直接插入查询结果, 应用场景, 查询结果过多, 不进行控制台打印直接插入到表里去
                 create table table_1 as select * from table_2 where table_2.age>20

            # 使用group by 进行嵌套查询 需要在后面添加as tablealis 不然会报错 这是在mysql的语法 
                 select count(1) from (select _from from table group by _from) as t;
                 select count(distinct(1)) from table;

            # 在hive中的使用 的性能比较
    修改表属性: 
        alter table iteblog set TBLPROPERTIES ('EXTERNAL'='TRUE');
        alter table iteblog set TBLPROPERTIES ('EXTERNAL'='FALSE');
  
    表的重新命名: 
       ALTER TABLE iteblog RENAME TO new_table_name
    加载数据到表中
       load data [local] inpath 'somepath' overwrite into table table_name partition (country='somevalue',state='somevalue');


命令2:
# 动态分区和静态分区
   静态分区:  一次导入到一个分区, 如果对应文件夹没有创建, 自动创建. 
   insert into table table_1 partition (year=2018,month=08,day=08) select * from table2 where year=2018 and month =08 and day =08
   静态分区一次导入到多个分区
        from table1 
        insert into table table2 partition (year=2018,month=08,day=08)
        select * where year=2018 and month=08 and day =08
        insert into table table2 partition (year=2018,month=08,day=09)
        select * where year=2018 and month=08 and day=09
       
   动态分区:
       动态分区默认是关闭的, 需要手动打开 容许动态分区, 容许所有列作为动态分区字段. hive.exec.dynamic.partition.mode 设置为nostrict
       根据字段自动创建分区,根据查询结果的后三位进行动态分区. 
       insert into table table2 partition(year,month,day) select * from t3.name,t3.age,.....,t3.year,t3.mohth,t3.day from table3 t3
       动态分区一次插入多张表中 from 字段放到前面可以插入到多张表中:  动态分区和静态分区结合使用 
       from table1
       insert into table table2 partition (year,month,day)
       select name,age,year,month,day
       insert into table table3 partition (year=2018,month=08,day)
       select name,age,year,month,day where table1.year=2018,month=08

命令3:
   join 操作
   inner join 内连接, 只用在两张表都符合where条件才会作为结果显示 ,join 只支持等值连接 on后面的条件使用'=’
   select a.f1,a.f2,b.f2 from table1 a join table2 b on a.f1=b.f1
   
   *** 待定  todo
   left outer join  左外连接  左边的表的记录符合where条件的都显示, 对应的右边的表

   union all
   把表合并到一起操作



#   hive 中array，map，struct字段类型的使用场景
	1，array
	        以电影数据为列
		数据维度
		导演，编剧，类型，主演，year，month，市场，score
		其中导演，编剧，类型和主演为array类型的
		创建表
		create table movie_message(
		    id int,
		    title string,
		    daoyan array<string>,
		    bianju array<string>,
		    leixing array<string>,
		    zhuyan array<string>,
		    year int,
	  	    month int,
            shichang int,
	        disnum int,
	        score float
		)
		comment "this table about movie's message"
		row format delimited fields terminated by ","
		collection items terminated by '/';
          加载本地数据：
	  load data local inpath "/home/master/mycode/new_movies_load.csv" into table movie_message;
	  内嵌查询及统计：
          select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;
		结果：
		        传记    194
		        儿童    18
			冒险    242
			剧情    1490
			动作    252
			动画    106
			历史    208
			古装    9
			同性    84
			喜剧    618
			奇幻    178
			家庭    130
			恐怖    152
			悬念    2
			悬疑    386

	保存结果到本地文件:
	insert overwrite local directory "/home/master/mycode/movie_leixing"
	row format delimited fields terminated by "\t"
	select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;

	2,map
	场景：
	create table people_movie(
		 name string,
		 movie map<string,string> )
		 row format delimited fields terminated by "\t"
		 collection items terminated by ","
		 map keys terminated by ":";
	加载数据：
		load data local inpath "/home/master/map" into table people_movie;

	hive> select * from people_movie;
	OK
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	Time taken: 0.148 seconds, Fetched: 4 row(s)
	hive> select movie['ABC'] from people_movie;
	OK
	2016-05
	NULL
	2016-05
	NULL
	使用explode关键字进行查询
	hive> select explode(movie) as (m_name,m_time) from people_movie;
	OK
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	使用explode和lateral view 结合查询
	hive> select name,mo,time from people_movie lateral view explode(movie) movie as mo,time;
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	
	3，Structs
	类似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分
	ABC     1254:7.4
	DEF     256:4.9
	XYZ     456:5.4
	
	创建表：
	create table movie_score(
    	> name string,
    	> info struct<number:int,score:float>)row format delimited fields terminated by "\t"
   	> collection items terminated by ":";


	hive> select * from movie_score;
	ABC     {"number":1254,"score":7.4}
	DEF     {"number":256,"score":4.9}
	XYZ     {"number":456,"score":5.4}
	hive> select info.number,info.score from movie_score;
	1254    7.4
	256     4.9
	456     5.4

	4，collect_set函数
        该函数的作用是将某字段的值进行去重汇总，产生Array类型字段
		
		select * from test;

		1       A
		1       C
		1       B
		2       B
		2       C
		2       D
		3       B
		3       C
		3	    d
	
	 select id,collect_set(name) from test group by id;
	  
	  1       ["A","C","B"]
	  2       ["B","C","D"]
	  3       ["B","C","D"]
    5,nvl 使用
       数据结构为
       A表: 
       id   name    age
       1    zhangsan    27
       2    lisi    28
       3    xiaoming    29
       5    sunchang    32
       6    lisi    29
       B表: 
       id   name    income
       2    lisi    5000
       3    xiaoming    4000
       4    wanghong    5000
       5    sunchang    0
       结果表结构为
       1    zhangsan    27  0
       2    lisi    28  5000
       3    xiaoming    29  4000
       4    wanghong    null    5000
       5    sunshang    32  0
       6    lisi    29  0
       方案1: 
        select a.id,a.name,a.age,nvl(b.income,0) from a left join b on a.id=b.id and a.name=b.name union all 
        select b.id,b.name,a.age,nvl(b.income,0) from b left join a on a.id=b.id and a.name=b.name where a.name is null
       方案2: 
        select nvl(a.id,b.id),nvl(a.name,b.name) ,a.age ,nvl(b.income,0) from a full join b on a.id=b.id and a.name=b.name
    6,explode 使用
        数据格式为: 
        start_day col1 col2 col3 col4 
        1991    1.1 1.2 1.3 1.4
        1992    2.1 2.2 2.3 2.4
        结果表: 
        1991    1   1.1
        1991    2   1.2
        1991    3   1.3
        1991    4   1.4
        1992    1   2.1
        1992    2   2.2
        1992    3   2.3
        1993    4   2.4
        方案
        select start_day,ss,rank() over(partition by start_day order by ss) from c lateral view explode(split(concat_ws(",",col1,col2,col3,col4),",")) b as ss
        lateral view explode 是一个udtf
        split('','')
        concat_ws('','')



******************************************************************hive 的优化************************************************
1: 
 设置hive本地模式运行
    set hive.exec.mode.local.auto=true
    有些语句就不会促发mapreduce执行, 比如selct * from table1;  select * from table where day=10 (分区表)
2: 
 尽量使用group by 进行分组操作, 效率高
3: 
 jvm重用 
       可以在 hadoop 的mapred-side.xml 文件中进行设置   
  <property>
    <name>mapred.job.reuse.jvm.num.tasks</name>
    <value>10</value>
  </property>
4:
 使用EXPLAIN 学习hive的工作原理，
   使用方式为：
	explain select sum(number) from oneclo;
   查看hql 是如何转换为mapreduce的
5：
 使用explain extended 显示更多的信息
6：
 限制调整
 limit 在很多情况下还是需要执行整个查看语句，然后返回部分结果的。
 hive 提供了一个配置属性可以开启，当使用limit语句时，其可以对源数据进行抽样
  <property>
	<name>hive.limit.optimize.enable</name>
	<value>true</value>
  </property>
  这个参数一旦打开，还有两个参数可以控制这个操作，分别为：hive.limit.row.max.size 和 hive.limit.optimize.limit.file
  这个功能的一个缺点是有可能输入的有用的数据永远不会被处理到，例如像任意的一个需要reduce步骤的查询，join和group by 操作，
     以及聚合函数的大多数调用，将会产生不同的结果。也许这个差异是可以接受的，但是重要的是要理解。
7：
 join 优化

8:
 并行执行
 hive 一次只会执行一个阶段，某个特定的job可能包含多个阶段，而这些阶段并非相互依赖，那么有些阶段是可以并行执行的，这样可以使得
 整个job的执行时间缩短，可以通过设置 hive.exec.parallel 为true ，开启并发执行。
9：
 严格模式
 设置属性 hive.mapred.mode 的值为strict 可以禁止3种类型的查询
 第一种，对于分区表，除非where 语句中包含有分区字段作为过滤条件来限制数据范围，否着不容许执行，即，不容许扫描所有的分区
 第二种，对于使用了order by 语句的查询，要求必须使用limit进行限制
 第三种，限制笛卡尔积的查询  join 后面没有使用on 
10：
 调整mapper和reducer个数
 保持平衡性非常有必要，如果有太多的mapper和reducer 任务，就会导致启动阶段调度和运行job过程中产生过多的开销，如果设置的太少，
 那么就有可能没有充分利用好集群内在的并行性。
       当执行的hive查询具有reduce 过程时，cli控制台会显示出优化后的reduces的个数，比如group by 操作，这种操作总是需要执行reduce  过程，
 hive 是按照输入的数据量的大小确定reducer 个数的，可以通过 dfs -count filepath 来计算出制定目录下的所有数据的总大小
 属性 hive.exec.reducers.bytes.per.reducer 默认值为1GB，一般情况下默认值是比较合适的，有些情况下map阶段产生的数据量过多，同样map
 阶段也可能会过滤掉输入数据集中的很大一部分的数据，而这时可能需要少量的reducer就满足了，可以通过设置mapred.reduce.tasks 为不同
 的值来确定是使用较多的还是较少的reducer，需要记住，受外部因素影响，这样的值会比较复杂设置，hadoop需要消耗好几秒的时间进行启动和 调度map和reduce任务，在进行性能测试的时候，要考虑到这些因素。
 属性 hive.exec.reducers.max 可以阻止某个查询消耗太多的资源 （集群总reducers 槽位个数*1.5）/（执行中查询的平均个数）
 1.5 是一个经验系数 
11:
 索引
    索引可以用来加快含有group by 语句的查询的计算速度。
12：
 动态分区调整
13：
 推测执行
14：
 单个mapreduce 中多个group by
15：
 虚拟列
 

 http://lxw1234.com/archives/2016/04/632.htm
 orc 索引row group index vs bloom filter index
 hive 的orc文件格式有着很高的压缩比, 还通过一个内置的轻量级索引, 提升查询性能, row group index 




********************************************************************************易混知识点****************************************
    hive 中order by, sort by, distribute by 和cluster by的介绍
    https://www.iteblog.com/archives/1534.html 有详细的介绍
    
    hive中的order by和mysql中的order by是一致的, 按照某一列, 或多列进行排序, 可以指定是正序或者是倒序, 它保证全局有序, 进行order by的时候会把所有数据发送到一个reducer中
    , 在大数据量的情况下可能不能接受, 这个操作最后会参数一个文件. 
    
    sort by 
    只能保证在同一个reduce中的数据按照指定的字段进行排序, 使用sort by 可以通过指定reduce的个数输出更多的数据, 对输出的数据再进行归并排序, 即可得到全部的结果, 需要注意的是n个reduce
    处理的数据范围是可以重叠的, 最后排序完的n个文件之间的数据范围是重叠的. 
    
    distribute by
    按照指定的字段将数据划分到不同的reduce中, 可以保证每个reduce中的数据范围不重叠, 每个分区内的数据是没有排序的.
    distribute by 一般和sort by一起使用: 进行reduce内部排序. 

    
    cluster by 具有distribute by 的功能, 并且可以在reduce中进行排序, 每个reduce内的数据是有序的, 可以达到全局有序的结果
********************************************************************************hive 中reduces个数是如何计算的************************
    参考文献: https://www.iteblog.com/archives/1697.html
 
 hive.exec.reducers.bytes.per.reducer
 此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是1G(1,000,000,000)；而从Hive 0.14.0开始，默认值变成了256M(256,000,000)，
 可以参见HIVE-7158和HIVE-7917。这个参数的含义是每个Reduce处理的字节数。比如输入文件的大小是1GB，那么会启动4个Reduce来处理数据 
 
 hive.exec.reducers.max 指定最多启动的reducer的个数
    此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是999；而从Hive 0.14.0开始，默认值变成了1009；可以参见HIVE-7158和HIVE-7917。
    这个参数的含义是最多启动的Reduce个数。比如input size/hive.exec.reducers.bytes.per.reducer>hive.exec.reducers.max，
    那么Hive启动的Reduce个数为hive.exec.reducers.max；反之为input size/hive.exec.reducers.bytes.per.reducer。
    这个参数只有在mapred.reduce.tasks/mapreduce.job.reduces设置为负数的时候才有效
 
 mapred.reduce.tasks/mapreduce.job.reduces
    此参数从Hive 0.1.0开始引入。默认值是-1。此参数的含义是Reduce的个数，典型的情况是设置成接近可用节点的质数。
    如果mapred.job.tracker的值是local此参数将会被忽略。在Hadoop中此参数的默认值是1；而在Hive中默认值是-1。
    通过将此参数设置为-1，Hive将自动计算出应该启动多少个Reduce。

 计算reduce个数的代码片段: 
    double bytes = Math.max(totalInputFileSize, bytesPerReducer); 
    int reducers = (int) Math.ceil(bytes / bytesPerReducer);
    reducers = Math.max(1, reducers);
    reducers = Math.min(maxReducers, reducers);   
 reduce 最小为1, 最大为maxReduces 就是hive.exec.reducers.max的值
    上面的代码仅仅是根据输入计算出reduce的个数, 最终的设置是通过设置setNumberOfReducers函数决定的
    如果job.getNumReduceTasks()>0，也就是mapred.reduce.tasks/mapreduce.job.reduces参数的值大于0，
    此时直接取mapred.reduce.tasks/mapreduce.job.reduces的值作为Reduce的个数；
    否则将会使用上面estimateNumberOfReducers函数估算Reduce个数，
    最后都是通过rWork.setNumReduceTasks(reducers)设置Reduce的个数。
总结: 
    1、Reduce的个数对整个作业的运行性能有很大影响。如果Reduce设置的过大，那么将会产生很多小文件，对NameNode会产生一定的影响，
       而且整个作业的运行时间未必会减少；如果Reduce设置的过小，那么单个Reduce处理的数据将会加大，很可能会引起OOM异常。
　　2、如果设置了mapred.reduce.tasks/mapreduce.job.reduces参数，那么Hive会直接使用它的值作为Reduce的个数；
　　3、如果mapred.reduce.tasks/mapreduce.job.reduces的值没有设置（也就是-1），那么Hive会根据输入文件的大小估算出Reduce的个数。
       根据输入文件估算Reduce的个数可能未必很准确，因为Reduce的输入是Map的输出，而Map的输出可能会比输入要小，所以最准确的数根据Map的输出估算Reduce的个数。

**********************************************************************************hive 分析窗口函数***************************************
   引用文献: http://lxw1234.com/archives/2015/04/181.htm  
             http://lxw1234.com/archives/2015/04/176.htm window 子句: 
    
   1,数据的准备和下面的一样: 
    SELECT cookieid,
    createtime,
    pv,
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1 
    SUM(pv) OVER(PARTITION BY cookieid) AS pv3,								--分组内所有行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv4,   --当前行+往前3行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv5,    --当前行+往前3行+往后1行
    SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv6   ---当前行+往后所有行  
    FROM lxw1234;
     
    cookieid createtime     pv      pv1     pv2     pv3     pv4     pv5      pv6 
    -----------------------------------------------------------------------------
    cookie1  2015-04-10      1       1       1       26      1       6       26
    cookie1  2015-04-11      5       6       6       26      6       13      25
    cookie1  2015-04-12      7       13      13      26      13      16      20
    cookie1  2015-04-13      3       16      16      26      16      18      13
    cookie1  2015-04-14      2       18      18      26      17      21      10
    cookie1  2015-04-15      4       22      22      26      16      20      8
    cookie1  2015-04-16      4       26      26      26      13      13      4
    
    如果不置顶rows between 那么默认从起点到当前行, 若果不指定order by 将分组内所有数据累加
    理解rows between的含义为: 
    preceding : 往前
    following : 往后
    current_row : 当前行
    unbounded : 起点 unbounded preceding:  表示从前面的起点, unbounded following : 表示到后面的终点 
    其它的AVG, min, max, 和sum的用法一样. 



    ntile,row_number,rank,dense_rank
    注意序列函数不支持window子句
            cookie1,2015-04-10,1
            cookie1,2015-04-11,5
            cookie1,2015-04-12,7
            cookie1,2015-04-13,3
            cookie1,2015-04-14,2
            cookie1,2015-04-15,4
            cookie1,2015-04-16,4
            cookie2,2015-04-10,2
            cookie2,2015-04-11,3
            cookie2,2015-04-12,5
            cookie2,2015-04-13,6
            cookie2,2015-04-14,3
            cookie2,2015-04-15,9
            cookie2,2015-04-16,7
             
            CREATE EXTERNAL TABLE lxw1234 (
            cookieid string,
            createtime string,   --day 
            pv INT
            ) ROW FORMAT DELIMITED 
            FIELDS TERMINATED BY ',' 
            stored as textfile location '/tmp/lxw11/';
   ntile(n) 用于将分组数据按照顺序切分为n片, 返回当前切片值, ntile不支持rows between  
   NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
   统计一个cookie pv最多的前三分之一的天, 
           select cookieid,createtime,pv,ntile(3) over(partition by cookieid order by pv desc) as rn from table;
           嵌套一下: 只要每一个cookieid的前三天的数据
        select * from (
             select cookieid,createtimr,pv,ntile(3) over(partition by cookieid order by pv desc) as rn from table
        )as tamp where tamp.rn=1;
        结果为:
        cookie1	2015-04-12	7	1
        cookie1	2015-04-11	5	1
        cookie1	2015-04-16	4	1
        cookie2	2015-04-15	9	1
        cookie2	2015-04-16	7	1
        cookie2	2015-04-13	6	1
   row_number() 从1开始, 按照顺序, 生成分组内记录的序列
   比如按照pv进行倒叙排序, 生成每天的pv名次 即使排名相等也会有顺序 1, 2, 3, 4, 5
        select cookieid,createtime,pv,row_number() over(partition by createtime order by pv desc) as rn from test1;
        按照时间找到每天的pv最大的cookieid 的相关记录
        select * from (select cookieid,createtime,pv,row_number() over(partition by createtime order by pv desc) as rn from test1) as tamp where tamp.rn=1;
   rank() 生成数据项在分组中的排名, 排名相等会在名次中留下空位 1, 2, 3, 3, 5
        select cookieid,createtime,pv,rank() over (partition by cookieid order by pv desc) as rn from test1;
   dense_rank() 和rank的区别, 排名相等的不会在次序的字段中留下空位, 1, 2, 3, 3, 4
        select cookieid,createtime,pv,dense_rank() over(partition by cookieid order by pv desc) as rn from test1; 
   







