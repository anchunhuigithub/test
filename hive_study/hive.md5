                                                       hive study file
1,version 1
    常用命令: 
    set hive.cli.print.current.db=true  设置出 cli的db名称
	set hive.cli.print.header=true   设置 cli的  查询结果显示出列名称
	create database databasename;      创建数据库
	create table tablename (filename type,filename1 type)j;      创建表

命令1:
# 把一个查询结果作为插入的内容插入到一张表里去 over 表示覆盖(先清空表，再插入)，into 表示直接插入。
     insert  overwrite/into table_target select * from table_src;
# 创建表的同时直接插入查询结果, 应用场景, 查询结果过多, 不进行控制台打印直接插入到表里去
     create table table_1 as select * from table_2 where table_2.age>20

# 使用group by 进行嵌套查询 需要在后面添加as tablealis 不然会报错 这是在mysql的语法 
     select count(1) from (select _from from table group by _from) as t;
     select count(distinct(1)) from table;

# 在hive中的使用 的性能比较

命令2:
# 动态分区和静态分区
   静态分区:  一次导入到一个分区, 如果对应文件夹没有创建, 自动创建. 
   insert into table table_1 partition (year=2018,month=08,day=08) select * from table2 where year=2018 and month =08 and day =08
    静态分区一次导入到多个分区
    from table1 
    insert into table table2 partition (year=2018,month=08,day=08)
    select * where year=2018 and month=08 and day =08
    insert into table table2 partition (year=2018,month=08,day=09)
    select * where year=2018 and month=08 and day=09
   
   动态分区:
   动态分区默认是关闭的, 需要手动打开 容许动态分区, 容许所有列作为动态分区字段. hive.exec.dynamic.partition.mode 设置为nostrict
   根据字段自动创建分区,根据查询结果的后三位进行动态分区. 
   insert into table table2 partition(year,month,day) select * from t3.name,t3.age,.....,t3.year,t3.mohth,t3.day from table3 t3
   动态分区一次插入多张表中 from 字段放到前面可以插入到多张表中:  动态分区和静态分区结合使用 
   from table1
   insert into table table2 partition (year,month,day)
   select name,age,year,month,day
   insert into table table3 partition (year=2018,month=08,day)
   select name,age,year,month,day where table1.year=2018,month=08

命令3:
   join 操作
   inner join 内连接, 只用两张表都符合where条件才会作为结果显示 ,join 只支持等值连接 on后面的条件使用'=’
   select a.f1,a.f2,b.f2 from table1 a join table2 b on a.f1=b.f1
   
   *** 待定  todo
   left outer join  左外连接  左边的表的记录符合where条件的都显示, 对应的右边的表






#       hive 中array，map，struct字段类型的使用场景
	1，array
	        以电影数据为列
		数据维度
		导演，编剧，类型，主演，year，month，市场，score
		其中导演，编剧，类型和主演为array类型的
		创建表
		create table movie_message(
		    id int,
		    title string,
		    daoyan array<string>,
		    bianju array<string>,
		    leixing array<string>,
		    zhuyan array<string>,
		    year int,
	  	    month int,
	            shichang int,
	            disnum int,
	            score float
		)
		comment "this table about movie's message"
		row format delimited fields terminated by ","
		collection items terminated by '/';
          加载本地数据：
	  load data local inpath "/home/master/mycode/new_movies_load.csv" into table movie_message;
	  内嵌查询及统计：
          select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;
		结果：
		        传记    194
		        儿童    18
			冒险    242
			剧情    1490
			动作    252
			动画    106
			历史    208
			古装    9
			同性    84
			喜剧    618
			奇幻    178
			家庭    130
			恐怖    152
			悬念    2
			悬疑    386

	保存结果到本地文件:
	insert overwrite local directory "/home/master/mycode/movie_leixing"
	row format delimited fields terminated by "\t"
	select lx,count(*) from movie_message lateral view explode(leixing) leixing as lx group by lx;

	2,map
	场景：
	create table people_movie(
		 name string,
		 movie map<string,string> )
		 row format delimited fields terminated by "\t"
		 collection items terminated by ","
		 map keys terminated by ":";
	加载数据：
		load data local inpath "/home/master/map" into table people_movie;

	hive> select * from people_movie;
	OK
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	A       {"ABC":"2016-05","EFG":"2016-09"}
	B       {"OPQ":"2015-06","XYZ":"2016-04"}
	Time taken: 0.148 seconds, Fetched: 4 row(s)
	hive> select movie['ABC'] from people_movie;
	OK
	2016-05
	NULL
	2016-05
	NULL
	使用explode关键字进行查询
	hive> select explode(movie) as (m_name,m_time) from people_movie;
	OK
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	ABC     2016-05
	EFG     2016-09
	OPQ     2015-06
	XYZ     2016-04
	使用explode和lateral view 结合查询
	hive> select name,mo,time from people_movie lateral view explode(movie) movie as mo,time;
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	A       ABC     2016-05
	A       EFG     2016-09
	B       OPQ     2015-06
	B       XYZ     2016-04
	
	3，Structs
	类似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分
	ABC     1254:7.4
	DEF     256:4.9
	XYZ     456:5.4
	
	创建表：
	create table movie_score(
    	> name string,
    	> info struct<number:int,score:float>)row format delimited fields terminated by "\t"
   	> collection items terminated by ":";


	hive> select * from movie_score;
	ABC     {"number":1254,"score":7.4}
	DEF     {"number":256,"score":4.9}
	XYZ     {"number":456,"score":5.4}
	hive> select info.number,info.score from movie_score;
	1254    7.4
	256     4.9
	456     5.4

	4，collect_set函数
        该函数的作用是将某字段的值进行去重汇总，产生Array类型字段
		
		select * from test;

		1       A
		1       C
		1       B
		2       B
		2       C
		2       D
		3       B
		3       C
		3	d
	
	 select id,collect_set(name) from test group by id;
	  
	  1       ["A","C","B"]
	  2       ["B","C","D"]
	  3       ["B","C","D"]
********************************************************************************优化************************************************
 
 设置hive本地模式运行
    set hive.exec.mode.local.auto=true
    有些语句就不会促发mapreduce执行, 比如selct * from table1;  select * from table where day=10 (分区表)
 
 尽量使用group by 进行分组操作, 效率高
 
 jvm重用



********************************************************************************易混知识点****************************************
    hive 中order by, sort by, distribute by 和cluster by的介绍
    https://www.iteblog.com/archives/1534.html 有详细的介绍
    
    hive中的order by和mysql中的order by是一致的, 按照某一列, 或多列进行排序, 可以指定是正序或者是倒序, 它保证全局有序, 进行order by的时候会把所有数据发送到一个reducer中
    , 在大数据量的情况下可能不能接受, 这个操作最后会参数一个文件. 
    
    sort by 
    只能保证在同一个reduce中的数据按照指定的字段进行排序, 使用sort by 可以通过指定reduce的个数输出更多的数据, 对输出的数据再进行归并排序, 即可得到全部的结果, 需要注意的是n个reduce
    处理的数据范围是可以重叠的, 最后排序完的n个文件之间的数据范围是重叠的. 
    
    distribute by
    按照指定的字段将数据划分到不同的reduce中, 可以保证每个reduce中的数据范围不重叠, 每个分区内的数据是没有排序的.
    
    cluster by 具有distribute by 的功能, 并且可以在reduce中进行排序, 每个reduce内的数据是有序的, 可以达到全局有序的结果
********************************************************************************hive 中reduces个数是如何计算的************************
    参考文献: https://www.iteblog.com/archives/1697.html
 
 hive.exec.reducers.bytes.per.reducer
 此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是1G(1,000,000,000)；而从Hive 0.14.0开始，默认值变成了256M(256,000,000)，
 可以参见HIVE-7158和HIVE-7917。这个参数的含义是每个Reduce处理的字节数。比如输入文件的大小是1GB，那么会启动4个Reduce来处理数据 
 
 hive.exec.reducers.max 指定最多启动的reducer的个数
    此参数从Hive 0.2.0开始引入。在Hive 0.14.0版本之前默认值是999；而从Hive 0.14.0开始，默认值变成了1009；可以参见HIVE-7158和HIVE-7917。
    这个参数的含义是最多启动的Reduce个数。比如input size/hive.exec.reducers.bytes.per.reducer>hive.exec.reducers.max，
    那么Hive启动的Reduce个数为hive.exec.reducers.max；反之为input size/hive.exec.reducers.bytes.per.reducer。
    这个参数只有在mapred.reduce.tasks/mapreduce.job.reduces设置为负数的时候才有效
 
 mapred.reduce.tasks/mapreduce.job.reduces
    此参数从Hive 0.1.0开始引入。默认值是-1。此参数的含义是Reduce的个数，典型的情况是设置成接近可用节点的质数。
    如果mapred.job.tracker的值是local此参数将会被忽略。在Hadoop中此参数的默认值是1；而在Hive中默认值是-1。
    通过将此参数设置为-1，Hive将自动计算出应该启动多少个Reduce。

 计算reduce个数的代码片段: 
    double bytes = Math.max(totalInputFileSize, bytesPerReducer); 
    int reducers = (int) Math.ceil(bytes / bytesPerReducer);
    reducers = Math.max(1, reducers);
    reducers = Math.min(maxReducers, reducers);   
 reduce 最小为1, 最大为maxReduces 就是hive.exec.reducers.max的值
    上面的代码仅仅是根据输入计算出reduce的个数, 最终的设置是通过设置setNumberOfReducers函数决定的
    如果job.getNumReduceTasks()>0，也就是mapred.reduce.tasks/mapreduce.job.reduces参数的值大于0，
    此时直接取mapred.reduce.tasks/mapreduce.job.reduces的值作为Reduce的个数；
    否则将会使用上面estimateNumberOfReducers函数估算Reduce个数，
    最后都是通过rWork.setNumReduceTasks(reducers)设置Reduce的个数。
总结: 
    1、Reduce的个数对整个作业的运行性能有很大影响。如果Reduce设置的过大，那么将会产生很多小文件，对NameNode会产生一定的影响，
       而且整个作业的运行时间未必会减少；如果Reduce设置的过小，那么单个Reduce处理的数据将会加大，很可能会引起OOM异常。
　　2、如果设置了mapred.reduce.tasks/mapreduce.job.reduces参数，那么Hive会直接使用它的值作为Reduce的个数；
　　3、如果mapred.reduce.tasks/mapreduce.job.reduces的值没有设置（也就是-1），那么Hive会根据输入文件的大小估算出Reduce的个数。
       根据输入文件估算Reduce的个数可能未必很准确，因为Reduce的输入是Map的输出，而Map的输出可能会比输入要小，所以最准确的数根据Map的输出估算Reduce的个数。














